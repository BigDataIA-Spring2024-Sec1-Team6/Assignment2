{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**This notebook explains - Uploading the csv file from Snowflake and the text files into the AWS S3 bucket**"
      ],
      "metadata": {
        "id": "yQysrfSJonfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Setting Up Snowflake"
      ],
      "metadata": {
        "id": "HZzN1Vyg6hUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Pre-requisites:**\n",
        "\n",
        "Installing snowflake-connector-python and boto3\n",
        "Before running the script, ensure that your environment is set up with the necessary Python packages.\n",
        "\n",
        "***Snowflake Connector for Python (snowflake-connector-python):*** This package allows to communicate with Snowflake from Python, executing queries and handling results.\n",
        "\n",
        "***Boto3:*** This is the Amazon Web Services (AWS) SDK for Python. It enables Python developers to create, configure, and manage AWS services, such as S3."
      ],
      "metadata": {
        "id": "4uOf4NUR6prh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#installing snowflake-connector-python and boto3 in cmd/ python book\n",
        "\n",
        "!pip install snowflake-connector-python boto3"
      ],
      "metadata": {
        "id": "MtSiDAlV7JtG",
        "outputId": "51c41662-a9b9-446f-9377-f9666b7945d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: snowflake-connector-python in /usr/local/lib/python3.10/dist-packages (3.7.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.34.43-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (1.5.1)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (1.16.0)\n",
            "Requirement already satisfied: cryptography<42.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (41.0.7)\n",
            "Requirement already satisfied: pyOpenSSL<24.0.0,>=16.2.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (23.3.0)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/lib/python3/dist-packages (from snowflake-connector-python) (2.3.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2023.4)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (4.9.0)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.13.1)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<4.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (3.11.0)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python) (0.12.3)\n",
            "Collecting botocore<1.35.0,>=1.34.43 (from boto3)\n",
            "  Downloading botocore-1.34.43-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
            "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.43->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.43->boto3) (2.0.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.43->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.34.43 botocore-1.34.43 jmespath-1.0.1 s3transfer-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Creating an S3 Stage in Snowflake**\n",
        "\n",
        "An **S3 stage** in Snowflake is a reference or pointer to an external location on Amazon S3 where you can stage (store temporarily) data files that are used for loading data into Snowflake or unloading data from Snowflake. It simplifies the process of specifying S3 paths in your SQL commands by allowing you to use the stage name instead of the full S3 path.\n",
        "\n",
        "To create an **S3 stage** that points to the S3 bucket, use Snowflake web interface or execute SQl command as below:\n"
      ],
      "metadata": {
        "id": "Dxkc4jP67x5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CREATE STAGE amos_s3_stage\n",
        "  URL='s3://assign2amos'\n",
        "  CREDENTIALS=(AWS_KEY_ID='<my_access_key>' AWS_SECRET_KEY='<my_secret_key>')\n",
        "  FILE_FORMAT = (TYPE = 'CSV');"
      ],
      "metadata": {
        "id": "v3F448qkI5Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Setting up AWS"
      ],
      "metadata": {
        "id": "mFT-seZgpGtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The setup includes creating an AWS account, setting up an S3 bucket, creating an IAM user with the necessary permissions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Step 1: Sign Up for AWS\n",
        "\n",
        "\n",
        "1. **Create an AWS Account:** Go to AWS homepage (https://aws.amazon.com/) and sign up.\n",
        "2. **Log in to the AWS Management Console:** Once the account is set up, log in to the AWS Management Console.\n",
        "\n",
        "### Step 2: Create an S3 Bucket\n",
        "\n",
        "\n",
        "1. **Go to S3 Service:** In the AWS Management Console, under Service menu find **S3**\n",
        "2. **Create a New Bucket:** Click **Create bucket**. Complete the creation of Bucket by following the instructions on-screen\n",
        "3. **Review and Create:** Review the settings and click **Create bucket**.\n",
        "4. **Bucket Name:** assign2amos\n",
        "\n",
        "### Step 3: Create an IAM User and Assign Permissions\n",
        "\n",
        "\n",
        "1. **Go to the IAM Service:** In the AWS Management Console, under the Service menu, find **IAM**\n",
        "2. **Create a New User:** In the navigation pane, click **Users**-->**Add user**. Provide a user name - **AMOS-teammates** and select Programmatic access for the AWS access type.\n",
        "3. **Attach Policies for S3 Access:** On the permissions page, select **Attach existing policies directly**. Search and select the `AmazonS3FullAccess` policy, or select the customized policy created - ***Policy_AMOS_uploadData.***\n",
        "4. **Review and Create:** Review the user details and permissions, then click on **Create user**. The AMOS-teammates user is now created.\n",
        "5. **Download Credentials:** After the user is created, download the **Access Key ID** and **Secret Access Key**. Save these credentials securel for later use.\n",
        "\n",
        "### Step 4: Configure AWS CLI with Your Credentials\n",
        "\n",
        "\n",
        "  1. **Install AWS CLI:** Download and install the AWS Command Line Interface (CLI)\n",
        "  2. **Configure the CLI:**\n",
        "    * Open a terminal or command prompt and run `aws configure`.\n",
        "    * Enter the access key ID and secret access key when prompted. Also, specify the default region name and output format (e.g., `us-west-2` and `json`).\n",
        "   ```\n",
        "   aws configure\n",
        "   AWS Access Key ID [None]: YOUR_ACCESS_KEY_ID\n",
        "   AWS Secret Access Key [None]: YOUR_SECRET_ACCESS_KEY\n",
        "   Default region name [None]: YOUR_PREFERRED_REGION\n",
        "   Default output format [None]: json\n",
        "   ```\n",
        "  3. **Verify Configuration:** Verify the  configuration, run `aws s3 ls` which will list the S3 buckets in your AWS\n",
        "\n",
        "\n",
        "### Step 5: Run Your Python Script\n",
        "\n",
        "\n",
        "After completion of the above steps, the AWS environment is ready.\n",
        "Now run the Python script below to upload files to your S3 bucket."
      ],
      "metadata": {
        "id": "XkXFRRum20LM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Uploading CSV file from Snowflake database to AWS S3 bucket"
      ],
      "metadata": {
        "id": "uGDEp1DraRqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code to upload the csv file from SNF to the AWS S3 bucket\n",
        "\n",
        "#working SNF to AWS S3 bucket Code:\n",
        "\n",
        "import snowflake.connector\n",
        "from snowflake.connector.pandas_tools import write_pandas\n",
        "\n",
        "# Snowflake connection parameters\n",
        "sf_account = 'CKMFZWO-AFA59273'\n",
        "sf_user = 'AKSHITAPATHANIA'\n",
        "sf_password = '<my_snf_password>'\n",
        "sf_warehouse = 'TEXT_EXTRACTION_WH'\n",
        "sf_database = 'TEXT_EXTRACTION'\n",
        "sf_schema = 'PUBLIC'\n",
        "sf_role = 'ACCOUNTADMIN'\n",
        "\n",
        "# AWS S3 parameters\n",
        "aws_access_key_id = '<my_access_key>'\n",
        "aws_secret_access_key = '<my-secret_key>'\n",
        "bucket_name = 'assign2amos'  #S3 bucket name\n",
        "s3_stage = 'amos_s3_stage'  # defined in Snowflake\n",
        "s3_key = 'CFA_SNF_Dataset/SNF_dataSet.csv'  # desired S3 key\n",
        "\n",
        "# SQL query to select data from Snowflake\n",
        "sql_query = \"\"\"\n",
        "SELECT *\n",
        "FROM TEXT_EXTRACTION.PUBLIC.STRUCTURED_DATA;\n",
        "\"\"\"\n",
        "\n",
        "# Connecting to Snowflake\n",
        "ctx = snowflake.connector.connect(\n",
        "    user=sf_user,\n",
        "    password=sf_password,\n",
        "    account=sf_account,\n",
        "    warehouse=sf_warehouse,\n",
        "    database=sf_database,\n",
        "    schema=sf_schema,\n",
        "    role=sf_role,\n",
        ")\n",
        "\n",
        "# Creating a cursor object\n",
        "cur = ctx.cursor()\n",
        "\n",
        "try:\n",
        "    # Use Snowflake's COPY INTO command to unload data to S3\n",
        "    unload_query = f\"\"\"\n",
        "    COPY INTO 's3://{bucket_name}/{s3_key}'\n",
        "    FROM TEXT_EXTRACTION.PUBLIC.STRUCTURED_DATA\n",
        "    CREDENTIALS = (AWS_KEY_ID='{aws_access_key_id}' AWS_SECRET_KEY='{aws_secret_access_key}')\n",
        "    FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY = '\\\"')\n",
        "    OVERWRITE = TRUE;\n",
        "    \"\"\"\n",
        "\n",
        "    # Executing the unload query\n",
        "    cur.execute(unload_query)\n",
        "\n",
        "    print(f\"Data successfully unloaded to S3 bucket '{bucket_name}' with key '{s3_key}'.\")\n",
        "\n",
        "finally:\n",
        "    # Closing the cursor and connection\n",
        "    cur.close()\n",
        "    ctx.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es7Q4AU0_J8v",
        "outputId": "752f3cae-5cf7-4332-87fa-8617fcf69605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully unloaded to S3 bucket 'assign2amos' with key 'CFA_SNF_Dataset/SNF_dataSet.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Uploading extracted text files from Grobid to AWS S3 Bucket"
      ],
      "metadata": {
        "id": "COctYnS7a2er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "\n",
        "def upload_files_to_s3(file_paths, aws_access_key_id, aws_secret_access_key, bucket_name, s3_keys):\n",
        "    \"\"\"\n",
        "    Uploads multiple files to an AWS S3 bucket.\n",
        "\n",
        "    Args:\n",
        "    - file_paths (list): Paths to the files to be uploaded.\n",
        "    - aws_access_key_id (str): AWS access key ID.\n",
        "    - aws_secret_access_key (str): AWS secret access key.\n",
        "    - bucket_name (str): Name of the AWS S3 bucket.\n",
        "    - s3_keys (list): The S3 keys (paths) for the uploaded files.\n",
        "    \"\"\"\n",
        "    # Initialize S3 client\n",
        "    s3_client = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
        "\n",
        "    for file_path, s3_key in zip(file_paths, s3_keys):\n",
        "        # Upload file to S3\n",
        "        s3_client.upload_file(file_path, bucket_name, s3_key)\n",
        "        print(f\"File '{file_path}' uploaded to S3 bucket '{bucket_name}' with key '{s3_key}'.\")\n",
        "\n",
        "# Assuming these files are text files and located in the current directory\n",
        "file_paths = [\n",
        "    'Grobid_RR_2024_l1_combined.txt',\n",
        "    'Grobid_RR_2024_l2_combined.txt',\n",
        "    'Grobid_RR_2024_l3_combined.txt'\n",
        "]\n",
        "\n",
        "s3_keys = [\n",
        "    'Grobid_Dataset/Grobid_RR_2024_l1_combined.txt',  # S3 keys for the Grodib data files\n",
        "    'Grobid_Dataset/Grobid_RR_2024_l2_combined.txt',\n",
        "    'Grobid_Dataset/Grobid_RR_2024_l3_combined.txt'\n",
        "]\n",
        "\n",
        "aws_access_key_id = '<my_access_key>'  # AWS access key ID\n",
        "aws_secret_access_key = '<my_secret_key>'  # AWS secret access key\n",
        "bucket_name = 'assign2amos'  # S3 bucket name\n",
        "\n",
        "\n",
        "upload_files_to_s3(file_paths, aws_access_key_id, aws_secret_access_key, bucket_name, s3_keys)\n"
      ],
      "metadata": {
        "id": "BrX0wNp5CTvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Uploading extracted text files from PyPDF to AWS S3 Bucket"
      ],
      "metadata": {
        "id": "ktq-Z6escnoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code to upload the text files - Grobid to the AWS S3 bucket\n",
        "\n",
        "import boto3\n",
        "\n",
        "def upload_files_to_s3(file_paths, aws_access_key_id, aws_secret_access_key, bucket_name, s3_keys):\n",
        "    \"\"\"\n",
        "    Uploads multiple files to an AWS S3 bucket.\n",
        "\n",
        "    Args:\n",
        "    - file_paths (list): Paths to the files to be uploaded.\n",
        "    - aws_access_key_id (str): AWS access key ID.\n",
        "    - aws_secret_access_key (str): AWS secret access key.\n",
        "    - bucket_name (str): Name of the AWS S3 bucket.\n",
        "    - s3_keys (list): The S3 keys (paths) for the uploaded files.\n",
        "    \"\"\"\n",
        "    # Initializing S3 client\n",
        "    s3_client = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
        "\n",
        "    for file_path, s3_key in zip(file_paths, s3_keys):\n",
        "        # Uploading file to S3\n",
        "        s3_client.upload_file(file_path, bucket_name, s3_key)\n",
        "        print(f\"File '{file_path}' uploaded to S3 bucket '{bucket_name}' with key '{s3_key}'.\")\n",
        "\n",
        "# Updating these paths if your files are located in a specific directory\n",
        "file_paths = [\n",
        "    'Pypdf_RR_2024_l1_combined.txt.txt',  # Adjusted for your specific files\n",
        "    'Pypdf_RR_2024_l2_combined.txt',\n",
        "    'Pypdf_RR_2024_l3_combined.txt.txt'\n",
        "]\n",
        "\n",
        "s3_keys = [\n",
        "    'pyPDF_Dataset/Pypdf_RR_2024_l1_combined.txt',  # Adjusted S3 keys for the new files\n",
        "    'pyPDF_Dataset/Pypdf_RR_2024_l2_combined.txt',\n",
        "    'pyPDF_Dataset/Pypdf_RR_2024_l3_combined.txt'\n",
        "]\n",
        "\n",
        "aws_access_key_id = '<my_access_key>'  # AWS access key ID\n",
        "aws_secret_access_key = '<my_secret_key>'  # AWS secret access key\n",
        "bucket_name = 'assign2amos'  # S3 bucket name\n",
        "\n",
        "upload_files_to_s3(file_paths, aws_access_key_id, aws_secret_access_key, bucket_name, s3_keys)"
      ],
      "metadata": {
        "id": "j1-dvAk7aPrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizing SQLAlchemy to upload the structured metadata from\n",
        "Grobid including the link to the uploaded text file (from S3) into a\n",
        "Snowflake database."
      ],
      "metadata": {
        "id": "G7OElDfjkXcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snowflake-sqlalchemy"
      ],
      "metadata": {
        "id": "J7TtHTJwkhza",
        "outputId": "359de73a-636d-469f-bc26-6a656c2207a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: snowflake-sqlalchemy in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: sqlalchemy<2.0.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-sqlalchemy) (1.4.51)\n",
            "Requirement already satisfied: snowflake-connector-python<4.0.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-sqlalchemy) (3.7.0)\n",
            "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (1.5.1)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (1.16.0)\n",
            "Requirement already satisfied: cryptography<42.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (41.0.7)\n",
            "Requirement already satisfied: pyOpenSSL<24.0.0,>=16.2.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (23.3.0)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/lib/python3/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2.3.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2023.4)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (4.9.0)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (3.13.1)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<4.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (3.11.0)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.10/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (0.12.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<2.0.0,>=1.4.0->snowflake-sqlalchemy) (3.0.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2.21)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2.0.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine\n",
        "from snowflake.sqlalchemy import URL\n",
        "\n",
        "engine = create_engine(URL(\n",
        "    user='akshitapathania',\n",
        "    password='<my_snf_password>',\n",
        "    account='CKMFZWO-AFA59273',\n",
        "    warehouse='GROBID_METADATA_WH',\n",
        "    database='GROBID_METADATA',\n",
        "    schema='PUBLIC'\n",
        "))\n"
      ],
      "metadata": {
        "id": "DYYn0wzn5S2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import Column, Integer, String, Sequence, create_engine\n",
        "from sqlalchemy.ext.declarative import declarative_base\n",
        "\n",
        "Base = declarative_base()\n",
        "\n",
        "class GrobidDocumentMetadata(Base):\n",
        "    __tablename__ = 'GROBID_DOCUMENT_METADATA'\n",
        "    id = Column(Integer, Sequence('doc_id_seq'), primary_key=True)\n",
        "    document_name = Column(String)\n",
        "    s3_link = Column(String)\n",
        "    additional_metadata = Column(String)\n"
      ],
      "metadata": {
        "id": "Kcsj0bqw4z81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Base.metadata.create_all(engine)"
      ],
      "metadata": {
        "id": "aDCqebGM7o8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy.orm import sessionmaker\n",
        "\n",
        "# Function to generate S3 link\n",
        "def generate_s3_link(bucket_name, s3_key):\n",
        "    return f\"s3://{bucket_name}/{s3_key}\"\n",
        "\n",
        "\n",
        "# Creating a session\n",
        "Session = sessionmaker(bind=engine)\n",
        "session = Session()\n",
        "\n",
        "\n",
        "bucket_name = 'assign2amos'\n",
        "\n",
        "# Metadata records to insert, based on your file upload code\n",
        "metadata_records = [\n",
        "    GrobidDocumentMetadata(\n",
        "        document_name='Grobid_RR_2024_l1_combined.txt',\n",
        "        s3_link=generate_s3_link(bucket_name, 'Grobid_Dataset/Grobid_RR_2024_l1_combined.txt'), #file 1 from grobid\n",
        "        additional_metadata='{}'\n",
        "    ),\n",
        "    GrobidDocumentMetadata(\n",
        "        document_name='Grobid_RR_2024_l2_combined.txt',\n",
        "        s3_link=generate_s3_link(bucket_name, 'Grobid_Dataset/Grobid_RR_2024_l2_combined.txt'), #file 2 from grobid\n",
        "        additional_metadata='{}'\n",
        "    ),\n",
        "    GrobidDocumentMetadata(\n",
        "        document_name='Grobid_RR_2024_l3_combined.txt',\n",
        "        s3_link=generate_s3_link(bucket_name, 'Grobid_Dataset/Grobid_RR_2024_l3_combined.txt'), #file 3 from grobid\n",
        "        additional_metadata='{}'\n",
        "    )\n",
        "]\n",
        "\n",
        "# Inserting the records into the metadata table\n",
        "session.add_all(metadata_records)\n",
        "session.commit()\n",
        "\n",
        "# Closing the session\n",
        "session.close()"
      ],
      "metadata": {
        "id": "YDUM3RtW5Njp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script creates entries in our Snowflake database for each document we've uploaded to S3, including their names, S3 links, and placeholders for any additional metadata from Grobid."
      ],
      "metadata": {
        "id": "Url_v03OurPk"
      }
    }
  ]
}
