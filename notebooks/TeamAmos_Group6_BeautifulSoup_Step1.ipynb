{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**This notebook explains - Webscraping the CFA Website using Beautiful Soup**"
      ],
      "metadata": {
        "id": "XJewdGL1XXfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction** :-\n",
        "\n",
        "What is Beautiful Soup?\n",
        "\n",
        "Beautiful Soup is a Python Library used for web scraping information from the website. Very simple to use, it parses the information from the HTML Structure.\n"
      ],
      "metadata": {
        "id": "BHxqTF16Xh4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries**"
      ],
      "metadata": {
        "id": "LlnCSXiTXR4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
        "import time\n"
      ],
      "metadata": {
        "id": "RIdSyhJrYma7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initializing the driver in headless mode**\n",
        "\n",
        "What is headless mode ?\n",
        "\n",
        "A chrome mode when you can run the browser without any visible UI."
      ],
      "metadata": {
        "id": "NxKp0hkIYvnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    driver.maximize_window()\n",
        "    return driver\n"
      ],
      "metadata": {
        "id": "BaewI5i7YvEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to close any Privacy Warnings :-**"
      ],
      "metadata": {
        "id": "BtaNF8WtZTiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def close_privacy_warning(driver):\n",
        "    try:\n",
        "        close_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"closePrivacyWarning\")))\n",
        "        close_button.click()\n",
        "    except (NoSuchElementException, TimeoutException):\n",
        "        print(\"Privacy warning not found.\")\n"
      ],
      "metadata": {
        "id": "jYN3fLUiZXMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to go the next button**\n",
        "\n",
        "For now, the sleep has been adjusted to 5 seconds, but if there's a timeout we can adjust that accordingly"
      ],
      "metadata": {
        "id": "hPqw5kLkZcPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def click_next_button(driver):\n",
        "    try:\n",
        "        next_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"coveo-pager-next\")))\n",
        "        next_button.click()\n",
        "        time.sleep(5)  # This can be adjusted to according to us\n",
        "        return True\n",
        "    except (NoSuchElementException, TimeoutException):\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "tVnIwKPPZbfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to scrape**\n",
        "\n",
        "Beautiful soup scrapes the HTML Structure of the website, so for now we are just trying to scrape the links and append in the our list"
      ],
      "metadata": {
        "id": "sgOqLpz1Zgfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape(driver, refresher_readings_list):\n",
        "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'coveo-title')))\n",
        "    html_content = driver.page_source\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    titles = soup.find_all('h4', class_='coveo-title')\n",
        "    for title in titles:\n",
        "        link = title.find('a')['href']\n",
        "        reading = [title.text.strip(), link]\n",
        "        refresher_readings_list.append(reading)\n"
      ],
      "metadata": {
        "id": "Om2QHILuZgo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to get Introduction and Learning Outcomes**\n",
        "\n",
        "After the inspection of the website, we realize that we could find Introduction in Article-section class but for Learning Outcomes it was nested within so we had to twist a logic a little"
      ],
      "metadata": {
        "id": "G4va6PZjZhzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reading_detail_data(driver, reading):\n",
        "    driver.get(reading[1])\n",
        "    try:\n",
        "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"section h2.article-section\")))\n",
        "    except TimeoutException:\n",
        "        print(\"Timeout waiting for section headers to load for reading:\", reading[0])\n",
        "        return {\"introduction\": \"\", \"learning_outcomes\": \"\", \"summary\": \"\"}, \"Unknown\", \"Unknown\", \"No PDF link found\"\n",
        "\n",
        "    html_content = driver.page_source\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    data = {\"introduction\": \"\", \"learning_outcomes\": \"\", \"summary\": \"\"}\n",
        "\n",
        "\n",
        "    for section in soup.find_all('section'):\n",
        "        header = section.find('h2', class_='article-section')\n",
        "        if header and 'Introduction' in header.text.strip():\n",
        "            data[\"introduction\"] = ' '.join(p.text.strip() for p in section.find_all('p'))\n",
        "\n",
        "    learning_outcomes_section = soup.find('h2', string='Learning Outcomes')\n",
        "    if learning_outcomes_section:\n",
        "        next_section = learning_outcomes_section.find_next_sibling('section')\n",
        "        if next_section:\n",
        "            outcomes_list = next_section.find('ul')\n",
        "            if outcomes_list:\n",
        "                data[\"learning_outcomes\"] = ' '.join(li.text.strip() for li in outcomes_list.find_all('li'))\n",
        "\n",
        "    year, level = extract_year_and_level(html_content)\n",
        "    pdf_link = get_pdf_link(soup)\n",
        "    return data, year, level, pdf_link\n",
        "\n",
        "    year, level = extract_year_and_level(html_content)\n",
        "    pdf_link = get_pdf_link(soup)\n",
        "    return data, year, level, pdf_link\n"
      ],
      "metadata": {
        "id": "WBS9aJc9Zh9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to get Year and Level**\n",
        "\n",
        "After the inspection of the website, we realize that we could find year  in content utility class and level there too"
      ],
      "metadata": {
        "id": "MWRqNy8xZw6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_year_and_level(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    content_utility_div = soup.find('div', class_='content-utility')\n",
        "    year_tag = content_utility_div.find('span', class_='content-utility-curriculum') if content_utility_div else None\n",
        "    level_tag = content_utility_div.find('span', class_='content-utility-topic') if content_utility_div else None  # Adjusted to target the correct span\n",
        "    year = year_tag.text.strip() if year_tag else \"Unknown\"\n",
        "    level = level_tag.text.strip() if level_tag else \"Unknown\"\n",
        "    return year, level\n"
      ],
      "metadata": {
        "id": "yyh0fuv5ZxCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to get Pdf link**\n",
        "\n",
        "After the inspection of the website,pdf link is found in the locked content class[link text](https://)"
      ],
      "metadata": {
        "id": "zgDJwfNOZzgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pdf_link(soup):\n",
        "    link_element = soup.find('a', class_='locked-content')  # Search for <a> tag with \"locked-content\" class\n",
        "    if link_element and 'href' in link_element.attrs:\n",
        "        return link_element['href']\n",
        "    return \"No PDF link found\"\n"
      ],
      "metadata": {
        "id": "v1heRuC4ZzpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to scrape reading detail and get it all structured**"
      ],
      "metadata": {
        "id": "xE8_3hnmZ1rP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_reading_detail(refresher_readings_list):\n",
        "    data_list = []\n",
        "    driver = initialize_driver()\n",
        "    for reading in refresher_readings_list:\n",
        "        reading_detail, year, level, pdf_link = get_reading_detail_data(driver, reading)\n",
        "        data_list.append({\n",
        "            'Title': reading[0],\n",
        "            'Year': year,\n",
        "            'Level': level,\n",
        "            'Introduction Summary': reading_detail['introduction'],\n",
        "            'Learning Outcomes': reading_detail['learning_outcomes'],\n",
        "#             'Summary': reading_detail['summary'],\n",
        "            'Link to Summary Page': reading[1],\n",
        "            'Link to PDF File': pdf_link\n",
        "        })\n",
        "    driver.quit()\n",
        "    return pd.DataFrame(data_list)\n"
      ],
      "metadata": {
        "id": "64j5_l3CZ17w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Function :-**"
      ],
      "metadata": {
        "id": "PqApcZkPaCuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    refresher_readings_list = []\n",
        "    driver = initialize_driver()\n",
        "    url = \"https://www.cfainstitute.org/en/membership/professional-development/refresher-readings#first=10&sort=%40refreadingcurriculumyear%20descending\"\n",
        "    driver.get(url)\n",
        "    close_privacy_warning(driver)\n",
        "    for page_num in range(23):  # Adjusted loop to check the return value of click_next_button\n",
        "        scrape(driver, refresher_readings_list)\n",
        "        if not click_next_button(driver):  # Check the return value to decide whether to continue\n",
        "            break  # Exit the loop if click_next_button returns False\n",
        "    df = scrape_reading_detail(refresher_readings_list)\n",
        "    print(df)\n",
        "    df.to_csv('refresher_readings.csv', index=False)\n",
        "    driver.quit()  # Ensure the driver is quit after the scraping is done\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "6v9TyE_yZ5H2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}